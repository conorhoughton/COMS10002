%4_recursion.tex
%notes for the course PandA1 COMS10002 taught at the University of Bristol
%2015 Conor Houghton conor.houghton@bristol.ac.uk

%To the extent possible under law, the author has dedicated all copyright 
%and related and neighboring rights to these notes to the public domain 
%worldwide. These notes are distributed without any warranty. 


\documentclass[11pt,a4paper]{scrartcl}
\typearea{12}
\usepackage{graphicx}
\usepackage{pstricks}
\usepackage{listings}
\lstset{language=C}
\pagestyle{headings}
\markright{COMS10002 - PandA1 4\_recursion - Conor}
\begin{document}

\subsection*{4 Recursion}

A process is recursive if it repeats itself in a self-similar way. An
easy example is the factorial
\begin{equation}
n!=n(n-1)(n-2)\ldots 1
\end{equation}
where $n$ is multiplied by $n-1$ and then by $n-2$ and so on down to one; or, put another way
\begin{equation}
n!=n(n-1)!
\end{equation}
and $1!=1$.

In computer science recursion refers to an approach where a problem is
solved by breaking it up into smaller similar problems. In practise
this often means solving a problem using a function that calls
itself. An easy example of recursion is given by the factorial; see
Table~\ref{c_factorial} or, for a fancier version
Table~\ref{c_factorial_fancy}. You will have learned that recursion is
often a good way to implement algorithms on computers, that many
algorithms can be designed recursively and that this is usually the
best way to program them. Being comfortable with recursion is one of
the signs of, and benefits of, learning to program properly! We will
also see that working out the big-oh complexity for many recursive
algorithms can be straight-forward using a set of formula known as
the master equation.

A recursive function consists of two parts, a \lq{}recursive
case\rq{}, what happens normally, here return n*factorial(n-1), and a
\lq{}terminating condition\rq{} or \lq{}base case\rq{}, a set of cases
that the algorithm will always arrive at and can deal with without
calling itself. These are important to avoid infinite recursion.

The factorial also provides a convenient example for discussing tail
recursion. One problem with recursion is that it can be wasteful of
stack memory, roughly speaking the memory the program uses to run. If
you call \texttt{factorial(10)} the program will write information
related to \texttt{factorial(10)}, \texttt{factorial(9)} and so on
down to \texttt{factorial(1)} onto the stack before
\texttt{factorial(1)} returns and the open functions get rolled, being
deleted from memory. In short, a copy of \texttt{factorial(9)} needs
to be stored while it waits for \texttt{factorial(8)} to return the
value 40320 to it, so that it can multiply that by 9 and return the
result, 362880, on to \texttt{factorial(10)}. This isn't really a
problem here, the factorial function will reach values well beyond the
capacity of int long long before the memory use on the stack becomes a
problem; however, in other circumstances it might be a problem. The
solution is tail recursion.

\begin{table}[b]
\begin{lstlisting}[numbers=left]
int factorial(int n)
{
   if(n<2)
      return 1;

   return n*factorial(n-1);
}

\end{lstlisting}
\caption{The recursive function for calculating $n!=n(n-1)\ldots 1$. If n<2 it returns 1, giving a terminating condition, it also means $0!=1$ which is a normal mathematical convention, otherwise it calls factorial(n-1). If you trying using this function, note that for even modest values of n, n! is too big to fit into int.\label{c_factorial}}
\end{table}


\begin{table}
\begin{lstlisting}[numbers=left]
int factorial(int n)
{
   return (n<2) ? 1 : n*factorial(n-1);
}

\end{lstlisting}
\caption{A fancier version of the factorial program which uses the ternary operator described in Table~\ref{c_ternary}.\label{c_factorial_fancy}}
\end{table}


\begin{table}
\begin{lstlisting}[numbers=left]
if (a)
   ans=b;
else 
   ans=c;
\end{lstlisting}
\caption{The ternary operator ans = a \& b : c evaluates a and then
  does either sets ans=b or ans=c depending on whether a is true of
  false.  Thus ans=a \& b : c is equivalent to the code above. Ternary
  operators are often faster to execute than the corresponding if
  statement.\label{c_ternary}}
\end{table}

An algorithm is tail recursive if the return value of the function
does not have to be modified before it is returned. The factorial
function in Table~\ref{c_factorial} is not tail recursive because
factorial(n-1) is multiplied by n before it is returned. However, the
version in Table~\ref{c_factorial_tail} is tail recursive, it manages
this by passing around another variable called big\_n that holds the
part of the factorial that is calculated so far. Now, when
\texttt{factorial(10)}, for example, is called it will call
\texttt{factorial(10,1)}, since $10>2$ this will call
\texttt{factorial(9,10)}; the only thing remaining for
\texttt{factorial(10,1)} to do is to return the value of
\texttt{factorial(9,10)} when it is done. This means, with a bit of
cunning, the function does not have to remain written on the stack,
the compiler just has to know that whatever \texttt{factorial(9,10)}
returns should be sent to whatever called \texttt{factorial(10)};
modern compilers are capable of this cunning so tail recursive
algorithms make more efficient use of stack memory.

\begin{table}
\begin{lstlisting}[numbers=left]
int factorial_r(int n, int big_n)
{
  if(n<2)
    return big_n;

  return factorial_r(n-1,n*big_n);
}

int factorial(int n)
{
  return factorial_r(n,1);
}
\end{lstlisting}
\caption{The tail recursive function for calculating $n!=n(n-1)\ldots
  1$. If n<2 it returns big\_n, otherwise it calls
  factorial(n-1,n*big\_n). Since nothing happens to the
  factorial(n-1,n*big\_n) before it is itself returned, this is an
  example of tail recursion.\label{c_factorial_tail}}
\end{table}

\begin{table}
\texttt{factorial\_r(10,1)}$\rightarrow$\texttt{factorial\_r(9,10)}$\rightarrow$\texttt{factorial\_r(8,90)}$\rightarrow$\\
\texttt{factorial\_r(7,720)}$\rightarrow$\texttt{factorial\_r(6,5040)}$\rightarrow$\texttt{factorial\_r(5,30240)}$\rightarrow$\\
\texttt{factorial\_r(4,151200)}$\rightarrow$\texttt{factorial\_r(3,604800)}$\rightarrow$\texttt{factorial\_r(2,1814400)}$\rightarrow$\\
\texttt{factorial\_r(1,3628800)}
\caption{The calling sequence of the tail recursive factorial program.\label{calling}}
\end{table}

Our task here is to calculate the algorithmic complexity of recursive
algorithms. For simplicity we will not worry about tail recursion,
though in practise, calculating the complexity for algorithms with
tail recursion is no harder.

The trick is to work out a recursive formula for $T(n)$, the run
time. Consider the factorial example; here
\begin{equation}
T(n)=c+T(n-1)
\end{equation}
where $c$ is a constant representing the computational time required
for factorial(n) itself, the if statement, the multiplication and so
on. Now we can expand it out
\begin{equation}
T(n)=c+[c+T(n-2)=2c+T(n-2)
\end{equation}
and this keeps going
\begin{equation}
T(n)=2c+T(n-2)=3c+T(n-3)=4c+T(n-4)=\ldots = (n-1)c+T(1)
\end{equation}
so factorial is $O(n)$. 

Another approach is to use an ansatz, that is to guess the answer. For
\begin{equation}
T(n)=c+T(n-1)
\end{equation}
we might guess from experience that this has a solution of the form
\begin{equation}
T(n)=An+B
\end{equation}
for some $A$ and $B$ we haven't specified yet. In fact $T(n)=An+B$
represents a whole family of possible solutions corresponding to different $A$s
and $B$s, we just need to show that family contains an actual solution. We do this by substitution. If we substitute into the recursion relation we get
\begin{equation}
An+B=c+A(n-1)+B=An+B+c-A
\end{equation}
which holds provided $A=c$. In otherwords we can make an educated
guess as to the form of the solution and then show by substitution
that there is a solution of this form.

We have already seen another recursive algorithm, although we didn't
write it as one: binary search. A recursive version of binary search is
given in Table~\ref{c_binary_search_recursive}. Here, leaving out
rounding effects and so on, in the worst case
\begin{equation}
T(n)=c+T(n/2)
\end{equation}
which is solved by 
\begin{equation}
T(n)=c\log_2(n)
\end{equation}
because 
\begin{equation}
T(n/2)=c\log_2(n/2)=c\log_2(n)-c\log_2(2)=c\log_2(n)-c
\end{equation}
and so, substituting back into the equation, this is the solution.
Here, again, working out the run time requires that you know how to
solve the recursion relation. Our approach here is to do what we have
been doing, we guess, based on the examples we've studied, and then
show we are correct by substituting back in.

\begin{table}
\begin{lstlisting}[numbers=left]
int search(int a[],int n, int val)
{
  return find_r(a,n,val,0,n-1);
}
 
int find_r(int a[],int n, int val,int low,int high)
{

  if(high<low)
    return -1;

  int mid=(high+low)/2;

  if(a[mid]==val)
    return mid;

  if(val>a[mid])
    return find_r(a,n,val,mid+1,high);
  
  return find_r(a,n,val,low,mid-1);
}
\end{lstlisting}
\caption{A recursive implementation of binary search. There are two
  halting conditions, val is found, or high<low, meaning that val
  isn't an element of a. Note that, though each call works with a
  smaller and smaller number of elements, for convenience the same
  array is used each time. This function is implemented in {\tt
    binary\_search\_recursive}.\label{c_binary_search_recursive}}
\end{table}

\end{document}
